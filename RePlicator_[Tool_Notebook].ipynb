{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RePlicator [Tool Notebook].ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNcnCi5PSZdm/RVgf4oQvbI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arindamkeswani/RePlicator/blob/main/RePlicator_%5BTool_Notebook%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UQAez_i-PLL"
      },
      "source": [
        "#**Description:**\n",
        "RePlicator (Relative Plagiarism Indicator) is a plagiarism indication tool meant to reduce the time taken to calculate level of plagiarism of a document with all the others in a dataset by using concepts of data-parallelism and libraries that work on similar principles.\n",
        "\n",
        "Application targets can include research facilities, universities, etc. who need a comparative analysis of a certain set of documents.\n",
        "\n",
        "Initially meant to be a research project, it was converted into an open-source tool with colab as its supporting platform, which enables users to use the tool **without worrying about licenses, hosting fees, and wasting memory on heavy applications.**\n",
        "\n",
        "\n",
        "Currently supported file types: \n",
        "- .pdf\n",
        "- .txt\n",
        "\n",
        "**All the user needs to do is run the code cells step-by-step (guide below).**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr_qZkqGLmzB"
      },
      "source": [
        "#**Research Summary:**\n",
        "- 4 methodologies were implemented, tested, and compared, with time as the deciding metric:\n",
        "  1. Serial implementation (for base time)\n",
        "  2. Psuedo data-parallelism (purely for research purposes)\n",
        "  3. Multiprocessing library (for base data-parallelism time)\n",
        "  4. Numba library (for potential in-built optimum time)\n",
        "- 5 Tests done on each methodology:\n",
        "  1. Miniature Data [3 mini-files]\n",
        "  2. Actual data [10 files]\n",
        "  3. Actual data [26 files] [In-built cosine similarity]\n",
        "  4. Actual data [26 files] [Manual cosine similarity]\n",
        "  5. Actual data [50 files] [Manual cosine similarity]\n",
        "- Mid-size dataset (20-30 files) saw a speed-up of `19.42%`, as compared to Serial time\n",
        "- Large-size dataset (50+ files) saw a speed-up of `28.74%`, as compared to Serial time\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_h_hxI4-THk"
      },
      "source": [
        "#Steps to use the tool:\n",
        "To run a code cell, simply press `[Shift + Enter]`\n",
        "\n",
        "Follow the below steps in sequence\n",
        "1. Click `Connect` on top right of the screen to **connect to the server**. You should see a green tick in a few seconds\n",
        "2. Run cell in `Section 0` to **Import necessary libraries** for the tool to work\n",
        "3. Run cell in `Section 1` to **Upload files**, when prompted\n",
        "4. Run cell in `Section 2` for **PDF file processing**\n",
        "5. Run cell in `Section 3` to **Check plagiarism**\n",
        "6. Run cell in `Section 4` to **Display full table**\n",
        "7. Run cell in `Section 5` to **Download full table**\n",
        "8. Run cell in `Section 6` to **Download summarized table**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXiexgXzDq6T"
      },
      "source": [
        "#**Section 0 : Import libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3_jI4vy-EC9"
      },
      "source": [
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "from numba import jit\n",
        "from numba import njit\n",
        "\n",
        "from os import system\n",
        "import sys\n",
        "from numpy import dot #Alt for cosine similarity\n",
        "from numpy.linalg import norm #Alt for cosine similarity\n",
        "\n",
        "import multiprocessing\n",
        "\n",
        "!pip install PyPDF2\n",
        "import PyPDF2 \n",
        "\n",
        "# !pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from google.colab import files \n",
        "\n",
        "\n",
        "print(\"__________________________________________\\n\\nALL LIBRARIES IMPORTED\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAz7XRE2Eg-L"
      },
      "source": [
        "#**Section 1 : Upload files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpVasLh-EiGn"
      },
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "print(\"__________________________________________\\n\\nALL FILES UPLOADED\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkv6TdEIFU9o"
      },
      "source": [
        "#**Section 2 : PDF File Processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1-Eo37xFdeI"
      },
      "source": [
        "student_files_pdf = [doc for doc in os.listdir() if doc.endswith('.pdf')]\n",
        "\n",
        "# %%time\n",
        "#Multiprocessing approach\n",
        "# creating a pdf file object \n",
        "def convert2(student_files_pdf):\n",
        "  for i in student_files_pdf:\n",
        "    try:\n",
        "      path=i\n",
        "      pin='/content/'+path\n",
        "      print(f\"Converting {pin.split('/')[-1]}...\")\n",
        "      pout=pin[:-4]+\".txt\"\n",
        "      print(pout)\n",
        "      pdfFileObj = open(pin, 'rb') \n",
        "          \n",
        "      # creating a pdf reader object \n",
        "      pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
        "          \n",
        "      # printing number of pages in pdf file \n",
        "      print(f\"Number of pages: {pdfReader.numPages}\") \n",
        "          \n",
        "      # creating a page object \n",
        "      s=\"\"\n",
        "      for i in range(pdfReader.numPages):\n",
        "        pageObj = pdfReader.getPage(i) \n",
        "          \n",
        "        # extracting text from page \n",
        "        \n",
        "        s+=pageObj.extractText()\n",
        "      print(f\"Writing contents of {pin} to {pout}\")\n",
        "      myText = open(pout,'w')\n",
        "\n",
        "      stop_words = set(stopwords.words('english'))\n",
        " \n",
        "      word_tokens = word_tokenize(s)\n",
        "      filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        " \n",
        "      filtered_sentence = []\n",
        "      \n",
        "      for w in word_tokens:\n",
        "          if w not in stop_words:\n",
        "              filtered_sentence.append(w)\n",
        "      \n",
        "      sp = \" \"\n",
        "  \n",
        "      # joins elements of list1 by '-'\n",
        "      # and stores in sting s\n",
        "\n",
        "      filter_joined = sp.join(filtered_sentence)\n",
        "      myText.write(filter_joined)\n",
        "      myText.close()\n",
        "      pdfFileObj.close()\n",
        "      print('_'*100)\n",
        "    except:\n",
        "      print(\"Cannot convert\",i)\n",
        "      print('_'*100)\n",
        "  \n",
        "pool = multiprocessing.Pool(processes=2) \n",
        "\n",
        "\n",
        "\n",
        "l1 = student_files_pdf[:len(student_files_pdf)//2]\n",
        "l2 = student_files_pdf[len(student_files_pdf)//2:]\n",
        "\n",
        "start=time.time()\n",
        "\n",
        "result = pool.map(convert2, [l1,l2])\n",
        "\n",
        "for i in result:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XXLJcrJFd2S"
      },
      "source": [
        "#**Section 3 : Check plagiarism**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwmfVPGXFkh1"
      },
      "source": [
        "student_files = [doc for doc in os.listdir() if doc.endswith('.txt')] #store all text files\n",
        "student_notes =[open(File).read() for File in  student_files] #stores all lines of all files\n",
        "\n",
        "vectorize = lambda Text: TfidfVectorizer().fit_transform(Text).toarray()  #to vectorize the words of text files\n",
        "\n",
        "vectors = vectorize(student_notes) #store vectorized values\n",
        "s_vectors = list(zip(student_files, vectors)) #store it with file names\n",
        "plagiarism_results =[]\n",
        "\n",
        "def check_plagiarism(s_vectors_partial):\n",
        "    # similarity = lambda doc1, doc2: cosine_similarity([doc1, doc2]) #to store similarity of two documents\n",
        "    plagiarism_results =[]\n",
        "    \n",
        "    sys.stdout.write(\"\\r\"+\"Starting process...\")\n",
        "    global s_vectors\n",
        "    for student_a, text_vector_a in s_vectors_partial:  #traverse through students and their vectors (for first document)\n",
        "        # print(f\"Started testing:{student_a}\")\n",
        "        # print(\"Started testing:\",student_a)\n",
        "        sys.stdout.write(\"\\r\"+\"Started testing:\"+student_a) # Cursor up one line\n",
        "        # time.sleep(1)\n",
        "        new_vectors = s_vectors.copy() \n",
        "        \n",
        "        # current_index = new_vectors.index((student_a, text_vector_a))\n",
        "        # del new_vectors[current_index]\n",
        "        \n",
        "\n",
        "        for student_b , text_vector_b in new_vectors: #traverse through students and their vectors (for second document)\n",
        "            # print(f\"Testing {student_a} against {student_b}\")\n",
        "            # print(\"Testing\",student_a,\"against\",student_b)\n",
        "            sys.stdout.write(\"\\r\"+\"Testing: \"+student_a+\" | Against: \"+student_b) # Cursor up one line\n",
        "            # time.sleep(1)\n",
        "            # sim_score = similarity(text_vector_a, text_vector_b)[0][1] #calculate similarity of both documents\n",
        "            sim_score = dot(text_vector_a, text_vector_b)/(norm(text_vector_a)*norm(text_vector_b))\n",
        "            # sim_score = cosine_similarity([text_vector_a, text_vector_b])[0][1]#########################Uncomment it later\n",
        "            # student_pair = sorted((student_a, student_b)) \n",
        "            student_pair = (student_a, student_b) \n",
        "            # score = (student_pair[0], student_pair[1],sim_score)\n",
        "            score = [student_pair[0], student_pair[1],float(\"%.2f\" % round(sim_score*100, 2))]\n",
        "            # plagiarism_results.add(score) #add score with file names into the set\n",
        "            plagiarism_results.append(score)\n",
        "            # print(\"Finished testing\",student_a,\"against\",student_b)\n",
        "        sys.stdout.write(\"\\r\"+\"Finished testing: \"+student_a)\n",
        "        sys.stdout.write(\"\\r\")\n",
        "        # print()\n",
        "    sys.stdout.write(\"\\r\"+\"TESTING COMPLETE!\")\n",
        "    return plagiarism_results  \n",
        "    # return createTable(plagiarism_results)\n",
        "\n",
        "def createTable(ans):\n",
        "    df=pd.DataFrame(np.zeros((len(student_files),len(student_files))),index=student_files,columns=student_files)\n",
        "\n",
        "    for data in ans:\n",
        "      for rowName in range(len(student_files)):\n",
        "        if df.index[rowName]==data[0]:\n",
        "          r=rowName\n",
        "          for colName in range(len(student_files)):\n",
        "            if df.index[colName]==data[1]:\n",
        "              c=colName\n",
        "\n",
        "              df.iloc[r,c] = data[2]\n",
        "    return df\n",
        "num_res= jit(parallel=True,forceobj=True)(check_plagiarism)\n",
        "ans=num_res(s_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rps8DrkvFk9O"
      },
      "source": [
        "#**Section 4 : Full table**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFyZ5aEMFoFf"
      },
      "source": [
        "df=createTable(ans)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO6hRNHxFpnj"
      },
      "source": [
        "#**Section 5 : Summary table**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWPeJwREFqzR"
      },
      "source": [
        "df_res=pd.DataFrame()\n",
        "df_res[\"Max plag value\"]= df.apply(lambda row: row.nlargest(2).values[-1],axis=1)\n",
        "df_res[\"Max plag doc\"]= df.T.apply(lambda x: x.nlargest(2).idxmin())\n",
        "df_res[\"Average plag\"] = (df.sum(axis=1)-1) / (len(df)-1)\n",
        "df_res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThSstsOFFsMg"
      },
      "source": [
        "#**Section 6 : Download full table**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbUY7nSEFtHk"
      },
      "source": [
        "df.to_csv('FullTable.csv')\n",
        "files.download('FullTable.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc-hYEt-Ftk6"
      },
      "source": [
        "#**Section 7 : Download summary table**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TibIOL5DFudP"
      },
      "source": [
        "df_res.to_csv('Summary.csv')\n",
        "files.download('Summary.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hX_2AO6FvhH"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}